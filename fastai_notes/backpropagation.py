# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_backprop.ipynb.

# %% auto 0
__all__ = ['path_data', 'path_gz', 'x_train', 'y_train', 'x_valid', 'y_valid', 'w1', 'b1', 'w2', 'b2', 'preds', 'model', 'loss',
           'l0', 'Relu', 'Lin', 'Mse', 'Model', 'Module', 'Linear']

# %% ../nbs/03_backprop.ipynb 2
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np
from pathlib import Path
from torch import tensor
from fastcore.test import test_close
torch.manual_seed(42)

mpl.rcParams['image.cmap'] = 'gray'
torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)
np.set_printoptions(precision=2, linewidth=125)

# dataloader & standardization as a tensor
path_data = Path('../data')
path_gz = path_data/'mnist.pkl.gz'
with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])

# %% ../nbs/03_backprop.ipynb 9
# define weights and biases these are generating random numbers off a normal distribution
w1 = torch.randn(m,nh)
b1 = torch.zeros(nh)
w2 = torch.randn(nh,1)
b2 = torch.zeros(1)

# %% ../nbs/03_backprop.ipynb 26
y_train,y_valid = y_train.float(),y_valid.float()

preds = model(x_train)
preds.shape

# %% ../nbs/03_backprop.ipynb 48
class Relu():
    """If the number is negative is to converted to 0
    
    Input: Output from the linear layer
    
    Adds an extra step of calculating the gradients with respect to the inputs.
    """
    def __call__(self, inp):
        self.inp = inp
        self.out = inp.clamp_min(0.)
        return self.out
    
    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g

# %% ../nbs/03_backprop.ipynb 49
class Lin():
    """
    Linear layer component of a MLP
    inputs are the weights and biases randomly initiated.
    
    Backward is moving the delta signal around to adjust the loss.
    """
    
    def __init__(self, w, b): self.w,self.b = w,b

    def __call__(self, inp):
        self.inp = inp
        self.out = lin(inp, self.w, self.b)
        return self.out

    def backward(self):
        self.inp.g = self.out.g @ self.w.t()
        self.w.g = self.inp.t() @ self.out.g
        self.b.g = self.out.g.sum(0)

# %% ../nbs/03_backprop.ipynb 50
class Mse():
    """
    Mean squared error this is the measure of success.
    
    input: the actual and prediction values from the linear layers
    backward stores the gradients and allows us to adjust the delta signal
    
    """
    def __call__(self, inp, targ):
        self.inp,self.targ = inp,targ
        self.out = mse(inp, targ)
        return self.out
    
    def backward(self):
        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]

# %% ../nbs/03_backprop.ipynb 51
class Model():
    """
    This is an object that combines the linear layer, relu and another linear layer (Making it a 3 layer MLP)
    input: weights, biases
    output: loss of the neural network
    
    """
    def __init__(self, w1, b1, w2, b2):
        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]
        self.loss = Mse()
        
    def __call__(self, x, targ):
        for l in self.layers: x = l(x)
        return self.loss(x, targ)
    
    def backward(self):
        self.loss.backward()
        for l in reversed(self.layers): l.backward()

# %% ../nbs/03_backprop.ipynb 57
class Module():
    """Compress representation"""
    def __call__(self, *args):
        self.args = args
        self.out = self.forward(*args)
        return self.out

    def forward(self): raise Exception('not implemented')
    def backward(self): self.bwd(self.out, *self.args)
    def bwd(self): raise Exception('not implemented')

# %% ../nbs/03_backprop.ipynb 58
class Relu(Module):
    def forward(self, inp): return inp.clamp_min(0.)
    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g

# %% ../nbs/03_backprop.ipynb 59
class Lin(Module):
    def __init__(self, w, b): self.w,self.b = w,b
    def forward(self, inp): return inp@self.w + self.b
    def bwd(self, out, inp):
        inp.g = self.out.g @ self.w.t()
        self.w.g = inp.t() @ self.out.g
        self.b.g = self.out.g.sum(0)

# %% ../nbs/03_backprop.ipynb 60
class Mse(Module):
    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()
    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]

# %% ../nbs/03_backprop.ipynb 67
# Pytorch
from torch import nn
import torch.nn.functional as F

# %% ../nbs/03_backprop.ipynb 68
class Linear(nn.Module):
    def __init__(self, n_in, n_out):
        super().__init__()
        self.w = torch.randn(n_in,n_out).requires_grad_()
        self.b = torch.zeros(n_out).requires_grad_()
    def forward(self, inp): return inp@self.w + self.b

# %% ../nbs/03_backprop.ipynb 69
class Model(nn.Module):
    def __init__(self, n_in, nh, n_out):
        super().__init__()
        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]
        
    def __call__(self, x, targ):
        for l in self.layers: x = l(x)
        return F.mse_loss(x, targ[:,None])

# %% ../nbs/03_backprop.ipynb 70
model = Model(m, nh, 1)
loss = model(x_train, y_train)
loss.backward()

# %% ../nbs/03_backprop.ipynb 71
l0 = model.layers[0] # take the output of the first linear layer
l0.b.grad

# %% ../nbs/03_backprop.ipynb 72
model.layers[2].w.grad
