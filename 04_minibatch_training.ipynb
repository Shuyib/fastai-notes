{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch import tensor,nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai utility to help with testing if the func\n",
    "from fastcore.test import test_close\n",
    "\n",
    "# edit formating of print statements\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "\n",
    "# set a random seed\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# customizing matplotlib plot style\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# define path to where the data is\n",
    "# loading the dataset into memory in validation sets and turning them into a tensor\n",
    "# notice it is put in a list for easy recovery\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the tuple: store the number of rows(n) and columns (m)\n",
    "n,m = x_train.shape\n",
    "\n",
    "# defining the target for the mnist to also include the highest number\n",
    "c = y_train.max()+1\n",
    "\n",
    "# nh means number of hidden layers\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class inheritance from nn.module\n",
    "# Helps make a MLP with 3 layers\n",
    "# idea emoji: the call enables you to call the class like a function e()\n",
    "# NB: the call allows us to index into the arch and get specific layers which are a list\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a MLP with 784 number of inputs(28 * 28) (m), 50 number of hidden layers (nh) and 10 number of outputs\n",
    "# instantiate that parameters\n",
    "model = Model(m, nh, 10)\n",
    "\n",
    "# apply to the feature set\n",
    "pred = model(x_train)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing into the arch\n",
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=784, out_features=50, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=50, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(model, \"layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.09, -0.21, -0.08,  ..., -0.03,  0.01,  0.06],\n",
       "        [-0.07, -0.14, -0.14,  ...,  0.03,  0.04,  0.14],\n",
       "        [-0.19, -0.04,  0.02,  ..., -0.01, -0.00,  0.02],\n",
       "        ...,\n",
       "        [-0.03, -0.22, -0.04,  ..., -0.01,  0.09,  0.14],\n",
       "        [-0.10, -0.09, -0.05,  ..., -0.01,  0.02,  0.11],\n",
       "        [-0.03, -0.25, -0.06,  ...,  0.00,  0.03,  0.14]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$ \n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just use the softmax we will get numbers in a small scale. We add the logarithm in order to fix the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use plot_function to see this\n",
    "from fastbook import *\n",
    "plot_function(log_softmax, min=0, max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "        ...,\n",
       "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(pred) # apply log "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True) #pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "        ...,\n",
       "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_close(logsumexp(pred), pred.logsumexp(-1))\n",
    "sm_pred = log_softmax(pred)\n",
    "sm_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "But since our $x$s are 1-hot encoded (actually, they're just the integer indices), this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n",
    "\n",
    "This can be done using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.48, -2.33, -2.28, -2.09, -2.36, -2.30, -2.38, -2.30, -2.30, -2.27], grad_fn=<SelectBackward0>),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[2], y_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.20, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.37, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.36, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing into arrays\n",
    "# remember they are 10 predictions for each array\n",
    "# we take the first array fifth element(its a 5, second element is a 0 and the third is 9\n",
    "# here we are grabbing a subset of the predictions\n",
    "sm_pred[0,5],sm_pred[1,0],sm_pred[2,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0,1,2], y_train[:3]] # compact way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative log loss\n",
    "# making the loss function\n",
    "# median gives the same answer but numbers are dropped in the calculation\n",
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement\n",
    "loss = nll(sm_pred, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use PyTorch's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3) # compare with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(F.cross_entropy(pred, y_train), loss, 1e-3) # they are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we run the log of softmax then do a negative log likelihood. This is called the **cross-entropy loss**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the training loop repeats over the following steps:\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>),\n",
       " torch.Size([50, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=50                  # batch size\n",
    "\n",
    "xb = x_train[0:bs]     # a mini-batch from x\n",
    "preds = model(xb)      # predictions\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 10]), torch.Size([50, 784]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, xb.shape # notices that number of rows is retained i.e the number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs] # grab the same batch size of the images\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb) #apply the loss function which is negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8, 3, 5, 9, 5, 9, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1) # on the predictions we find the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.08)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb) # 80% accuracy on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5   # learning rate\n",
    "epochs = 3 # how many epochs to train for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the output of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.30, 0.08\n"
     ]
    }
   ],
   "source": [
    "# take a batch of Xs and ys\n",
    "# store them in variables\n",
    "xb,yb = x_train[:bs],y_train[:bs]\n",
    "\n",
    "# apply the MLP\n",
    "preds = model(xb)\n",
    "\n",
    "# see results formatted in a better manner\n",
    "report(loss_func(preds, yb), preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12, 0.98\n",
      "0.12, 0.94\n",
      "0.08, 0.96\n"
     ]
    }
   ],
   "source": [
    "# for the whole dataset\n",
    "# minibatch training\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n,i+bs)) # helps to grab a mini batch\n",
    "        xb,yb = x_train[s],y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer section: Stochastic gradient descent\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr # update the weights\n",
    "                    l.bias   -= l.bias.grad   * lr # update the bias\n",
    "                    l.weight.grad.zero_() # zeroing gradients\n",
    "                    l.bias  .grad.zero_() # zeroing bias\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parameters and optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate nn.module used by NN\n",
    "m1 = nn.Module() \n",
    "\n",
    "# name the linear layer foo\n",
    "m1.foo = nn.Linear(3,4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', Linear(in_features=3, out_features=4, bias=True))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m1.named_children()) # get the named linear layer called foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_children at 0x7ff4378db220>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.named_children() # returns a generator without list command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.44,  0.48, -0.14],\n",
       "         [ 0.53, -0.13,  0.12],\n",
       "         [-0.28,  0.34,  0.51],\n",
       "         [-0.42,  0.50,  0.11]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.43,  0.08,  0.28, -0.08], requires_grad=True)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(m1.parameters()) # see the parameters of the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class inheritance and write our own nn.Module MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x): return self.l2(self.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the arguments to the MLP\n",
    "model = MLP(m, nh, 10)\n",
    "\n",
    "# we use dot notation to grab the parameter\n",
    "model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # see the named module as class name & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n",
      "relu: ReLU()\n"
     ]
    }
   ],
   "source": [
    "#  iterate over the model children and print the named parameters\n",
    "for name,l in model.named_children(): print(f\"{name}: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters(): print(p.shape) # apply function over named parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit function responsible for the training loop\n",
    "def fit():\n",
    "    for epoch in range(epochs): # training step\n",
    "        for i in range(0, n, bs): # do training on the batch\n",
    "            s = slice(i, min(n,i+bs)) # pick a slice of the data\n",
    "            xb,yb = x_train[s],y_train[s] # grab a batch of the feature and target same section\n",
    "            preds = model(xb) # predict on the batch \n",
    "            loss = loss_func(preds, yb) # calculate the loss function\n",
    "            loss.backward() # backprop\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr # adjust weights and multiply by learning rate\n",
    "                model.zero_grad() # zero the gradients\n",
    "        report(loss, preds, yb) # see how you are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12, 0.96\n",
      "0.07, 0.98\n",
      "0.05, 0.98\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule:\n",
    "    def __init__(self, n_in, nh, n_out): # defines regular MLP storing parameters in dict\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "\n",
    "    def __setattr__(self,k,v): # picking certain parameters startswith\n",
    "        if not k.startswith(\"_\"): self._modules[k] = v\n",
    "        super().__setattr__(k,v)\n",
    "\n",
    "    def __repr__(self): return f'{self._modules}' # allows better printing of objects\n",
    "    \n",
    "    def parameters(self): # loop in _modules and see the parameters\n",
    "        for l in self._modules.values(): yield from l.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = MyModule(m,nh,10)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in mdl.parameters(): print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the original `layers` approach, but we have to register the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "\n",
    "    def forward(self, x): return reduce(lambda val,layer: layer(val), self.layers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.ModuleList` does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module): # moving to sequential model\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15, 0.94\n",
      "0.11, 0.98\n",
      "0.10, 0.98\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` is a convenient class which does the same as the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we were building the previous naming of the manual naming to using pytorch\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16, 0.94\n",
      "0.14, 0.96\n",
      "0.12, 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.02, grad_fn=<NllLossBackward0>), tensor(1.))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model update step: SGD \n",
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16, 0.96\n",
      "0.10, 0.96\n",
      "0.05, 0.98\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n,i+bs))\n",
    "        xb,yb = x_train[s],y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress model into a function save the model and results\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, 0.98\n",
      "0.06, 1.00\n",
      "0.03, 1.00\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n,i+bs))\n",
    "        xb,yb = x_train[s],y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clunky to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[s]\n",
    "    yb = y_train[s]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[s]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y # stores value of x and y in a tuple\n",
    "    def __len__(self): return len(self.x) # find the number of items in x\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i] # get the items in x and y via indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the class: use the xtrain,ytrain and validation set\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "\n",
    "# confirm if the structure is the same in both datasets\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a batch of 5 features and labels\n",
    "xb,yb = train_ds[0:5]\n",
    "\n",
    "# check if the shape is 5 & 28 by 28\n",
    "assert xb.shape==(5,28*28)\n",
    "\n",
    "# if the y has 5 items/\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model() # run MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15, 0.92\n",
      "0.12, 0.96\n",
      "0.07, 0.96\n"
     ]
    }
   ],
   "source": [
    "# run the training loop\n",
    "for epoch in range(epochs): # in each epoch\n",
    "    for i in range(0, n, bs): # using a particular batch\n",
    "        xb,yb = train_ds[i:min(n,i+bs)] # grab defined n to batch size\n",
    "        preds = model(xb) # run MLP\n",
    "        loss = loss_func(preds, yb) # calculate loss function\n",
    "        loss.backward() # backprop\n",
    "        opt.step() # update weights\n",
    "        opt.zero_grad() # zero gradients\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "\n",
    "```python\n",
    "for i in range(0, n, bs):\n",
    "    xb,yb = train_ds[i:min(n,i+bs)]\n",
    "    ...\n",
    "```\n",
    "\n",
    "Let's make our loop much cleaner, using a data loader:\n",
    "\n",
    "```python\n",
    "for xb,yb in train_dl:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up the dataset class with a iterator and list indexing\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the size of the validation set rows and columns\n",
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3, 8, 3, 7, 7, 8, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb # all the targets like we wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view feature and the label\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb,yb in train_dl: # changed here more concise\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14, 0.96\n",
      "0.10, 0.96\n",
      "0.08, 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.03, grad_fn=<NllLossBackward0>), tensor(1.))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit() # training loop\n",
    "\n",
    "# measures of success\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(): # single sampler\n",
    "    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle # initialize the class to have a set shuffle value and is able to calculate the len\n",
    "    def __iter__(self):\n",
    "        res = list(range(self.n)) # convert range to list\n",
    "        if self.shuffle: random.shuffle(res) # confirms the value of shuffle and rearranges the list\n",
    "        return iter(res) # enables iteration over the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = Sampler(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "it = iter(ss)\n",
    "for o in range(5): print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(train_ds, shuffle=False)\n",
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(): # multiple sampler\n",
    "    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n",
    "    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [4, 5, 6, 7],\n",
       " [8, 9, 10, 11],\n",
       " [12, 13, 14, 15],\n",
       " [16, 17, 18, 19]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchs = BatchSampler(ss, 4)\n",
    "list(islice(batchs, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b): # collect xs and ys into a single matrix\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()\n",
    "    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(Sampler(train_ds, shuffle=True ), bs)\n",
    "valid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batchs=train_samp)\n",
    "valid_dl = DataLoader(valid_ds, batchs=valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 784]), torch.Size([50]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18, 0.06\n",
      "0.03, 0.06\n",
      "0.20, 0.04\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from fastcore.basics import store_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 1, 1, 0]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access elements in the dataset\n",
    "train_ds[[3,6,8,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 1, 1, 0]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way of accessing elements in the dataset\n",
    "train_ds.__getitem__([3,6,8,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# map the dataset to the index\n",
    "for o in map(train_ds.__getitem__, ([3,6],[8,1])): print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    '''\n",
    "    dataloaders are used to load data in batches and use multiprocessing to speed up the process\n",
    "\n",
    "    ds: dataset\n",
    "    batchs: batch sampler\n",
    "    n_workers: number of workers to use\n",
    "    collate_fn: function to collate the data into x and y\n",
    "\n",
    "    '''\n",
    "    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()\n",
    "    def __iter__(self):\n",
    "        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the dataloader\n",
    "train_dl = DataLoader(train_ds, batchs=train_samp, n_workers=2)\n",
    "it = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 784]), torch.Size([50]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first batch\n",
    "xb,yb = next(it)\n",
    "\n",
    "# check the shape of the batch\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader with BatchSampler and RandomSampler\n",
    "# batch sampler is used to create batches of data, random sampler is used to shuffle the data\n",
    "train_samp = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\n",
    "valid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataloader: collate_fn is used to collate the data into x and y batches in train and validation sets\n",
    "train_dl = DataLoader(train_ds, batch_sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15, 0.06\n",
      "0.31, 0.20\n",
      "0.03, 0.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.02, grad_fn=<NllLossBackward0>), tensor(1.))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the training loop: this is the same as before\n",
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can auto-generate the BatchSampler for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can also generate the Sequential/RandomSamplers too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, 0.10\n",
      "0.39, 0.14\n",
      "0.03, 0.08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.02, grad_fn=<NllLossBackward0>), tensor(1.))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset actually already knows how to sample a batch of indices all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([9, 1, 3]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[[4,6,7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...that means that we can actually skip the batch_sampler and collate_fn entirely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader easily\n",
    "train_dl = DataLoader(train_ds, sampler=train_samp)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl): # training loop\n",
    "    for epoch in range(epochs): # iterate over the number of epochs\n",
    "        model.train() # set the model to training mode\n",
    "        for xb,yb in train_dl: # iterate over the training set\n",
    "            loss = loss_func(model(xb), yb) # calculate loss\n",
    "            loss.backward() # calculate gradients which are stored in the .grad attribute of each parameter\n",
    "            opt.step() # update the parameters\n",
    "            opt.zero_grad() # reset the gradients to zero\n",
    "\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad(): # disable gradient calculation\n",
    "            tot_loss,tot_acc,count = 0.,0.,0 # initialize the loss, accuracy and count\n",
    "            for xb,yb in valid_dl: # iterate over the validation set\n",
    "                pred = model(xb) # get the predictions\n",
    "                n = len(xb) # get the number of samples in the batch\n",
    "                count += n # add the number of samples to the count variable\n",
    "                tot_loss += loss_func(pred,yb).item()*n # calculate the loss and add it to the total loss\n",
    "                tot_acc  += accuracy (pred,yb).item()*n # calculate the accuracy and add it to the total accuracy\n",
    "        print(epoch, tot_loss/count, tot_acc/count) # print the epoch, loss and accuracy\n",
    "    return tot_loss/count, tot_acc/count # return the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs): # create dataloaders\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs), # training dataloader\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs)) # validation dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed version of the training loop\n",
    "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.18484509313479067 0.9427000033855438\n",
      "1 0.13305967260152102 0.960700004696846\n",
      "2 0.15698771580122411 0.9535000020265579\n",
      "3 0.10447211955441162 0.96760000705719\n",
      "4 0.10551201710710302 0.9704000061750412\n",
      "CPU times: user 30.5 s, sys: 302 ms, total: 30.9 s\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%time loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl) # in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of notebook\n",
    "# We created a training loop that can be used to train a model. \n",
    "# We also created a function to create dataloaders that can be used to load data in batches.\n",
    "# We used the dataloaders to create training and validation dataloaders.\n",
    "# We used the training loop to train a model on the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cf35bb1f5e149d9f3d68c32e9da2c66d2b9890c3c03aecbc85e754d546101c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
